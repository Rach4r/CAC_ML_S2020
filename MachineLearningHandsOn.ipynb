{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income prediction using Machine Learning\n",
    "\n",
    "In this tutorial, you explore developing machine learning models using Python.\n",
    "The data set that will be used is called UCI: Adult – Predict Income. \n",
    "This data set is meant to be used to predict whether an individual has an income of less than 50K or more than 50K based on census data. \n",
    "\n",
    "## In this notebook\n",
    "\n",
    " - Find the API Docs for the running version of Pandas & scikit-learn\n",
    " - Run data exploration\n",
    " - Run data visualization\n",
    " - Run data preparation\n",
    " - Train models\n",
    " - Evaluate models\n",
    " - Save and load trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what data we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Pandas & Scikit-learn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV to a Panda's Dataframe\n",
    "\n",
    "filePath = 'adult.csv'\n",
    " \n",
    "df_data_1=pd.read_csv(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using head() method with an argument to display more rows of the dataset\n",
    "df_data_1.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Notice that row 14 in the dataset contains a question mark for an unknown value, many other rows in the data set are also missing values. Later on when prepossessing the data, all rows that contain missing values will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tail() to display last rows of the dataset\n",
    "df_data_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dtypes to display the datatypes of each column \n",
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using describe to display the summary statistics of the numeric columns \n",
    "df_data_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn will be used to create plots in order to visualize the dataset. Using Seaborn many different types of plots can be created. To browse the available visualization types, visit the Seaborn gallery at https://seaborn.pydata.org/examples/index.html. In this exercise two types of plots will be used, a count plot and a violin plot. \n",
    "\n",
    "1.\tImport the Seaborn library and import pyplot from the matplotlib module.\n",
    "2.\tAssign a value to the style parameter in the Seaborn set method to change the appearance of the plot. The five Seaborn styles are darkgrid, whitegrid, dark, white and ticks. In the example below darkgrid is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#set the plot theme\n",
    "sb.set(style = \"darkgrid\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tCreate a count plot using the countplot method to see the number of males and females in each income category. Have SEX as the x value and SALARY as the hue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a count plot\n",
    "sb.countplot('sex', data=df_data_1, hue = 'salary')\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tCreate a violin plot using the violinplot method to see the age distribution for each income category.  Have SALARY as the x value, AGE as the y value, and the df_data_1 as the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and disply a violin plot\n",
    "sb.violinplot(x = \"salary\", y = \"age\", data = df_data_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tSet the hue of the violin plot to SEX and set split to True to see the age distribution based on gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and display a violin plot\n",
    "sb.violinplot(x = \"salary\", y = \"age\", hue=\"sex\", data = df_data_1, split=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find correlation between columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(df, size=15):\n",
    "    corr= df.corr()\n",
    "    fig, ax =plt.subplots(figsize=(size,size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(df_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tNow, we will be preparing to build a prediction model using scikit-learn. First, we need to clean up the data.  Recall how some rows contained ‘ ?’ instead of a value. We will begin by removing these rows.\n",
    "\n",
    "I.\tImport numpy\n",
    "\n",
    "II.\tThree different columns, workclass, occupation and native_country, contain ‘ ?’. Mark values that contain ‘ ?’ as  missing by replacing the ‘ ?’ with NaN. \n",
    "\n",
    "III.\tDrop all rows that contain missing values using the dropna method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_data_1['workclass'].value_counts(), df_data_1['occupation'].value_counts(), df_data_1['country'].value_counts()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# mark ' ?' values as missing or NaN\n",
    "df_data_1['workclass'] = df_data_1['workclass'].replace(' ?', numpy.NaN)\n",
    "df_data_1['occupation'] = df_data_1['occupation'].replace(' ?', numpy.NaN)\n",
    "df_data_1['country'] = df_data_1['country'].replace(' ?', numpy.NaN)\n",
    "\n",
    "# drop rows with missing values\n",
    "df_data_1.dropna(inplace=True)\n",
    "\n",
    "[df_data_1['workclass'].value_counts(), df_data_1['occupation'].value_counts(), df_data_1['country'].value_counts()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tAlso recall how many of the columns in the dataset contained Object (String) data values. We are now going to convert these values to lower case. For each String column use pandas map method to apply the lower method to all record. Use the head method to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all String values to lowercase\n",
    "string_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship',  'race', 'sex', 'country', 'salary']\n",
    "\n",
    "for col in string_columns:\n",
    "    df_data_1[col] = df_data_1[col].map(lambda x: x.lower())\n",
    "\n",
    "#display the initial records that are now lowercase\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tConvert the columns that have Object/String datatypes into numeric values using dummy encoding. Reminder: to see which columns are Strings you can use the dtype method described earlier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert String Columns to numeric using one hot encoding\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "all_numeric_df = pd.concat([df_data_1,pd.get_dummies(df_data_1[string_columns], prefix=string_columns, drop_first=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numeric_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numeric_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop the original String column (you don't need it anymore)\n",
    "all_numeric_df.drop(string_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numeric_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tInstead of transforming all attributes to numeric as in the previous step, we can also transform all attributes to Categorical/String types depending on the machine learning algorithm we are using. Try this on the age attribute. Instead of having age values ranging from 17 to 90, break the values into ten bins.  \n",
    "\n",
    "I.\tInstantiate the KBinsDiscretizer Object. Set n_bin to 10, encode to ordinal, and strategy to uniform. \n",
    "\n",
    "II.\tCall the fit_transform method on the values in the AGE column. Print the results and notice that the values are assigned a bin from 0 to 9 based on how high the number is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#create an instance of the KBinsDiscretizer Object\n",
    "bd = preprocessing.KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "\n",
    "#bin continuous data into intervals and print the result\n",
    "print(bd.fit_transform([[x] for x in df_data_1['age']]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For the rest of this exercise we will be using the original numeric attributes, however, if we had wanted to update the values in the dataset we could have assigned the result to df_data_1['AGE'], instead of printing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set & Test Set\n",
    "1.\tSeparate the data into feature and target variables. \n",
    "\n",
    "I.\tThe PREDICTION column will be the target set. \n",
    "\n",
    "II.\tSelect all columns other than the PREDICTION column and assign them to a variable called data. \n",
    "\n",
    "III.\tAssign the target set equal to the PREDICTION column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features columns to a DF variable\n",
    "numeric_features_df = all_numeric_df.loc[:, all_numeric_df.columns != 'salary_ >50k']\n",
    "\n",
    "#Set target set equal to prediction column\n",
    "target = all_numeric_df['salary_ >50k']\n",
    "\n",
    "numeric_features_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tSplit the data set into training and testing sets, with 20% of the data being used as the test data and 80% used as the training data.  \n",
    "\n",
    "I.\tImport the train_test_split model from sklearn.model_selection.\n",
    "\n",
    "II.\tSplit the data using the train_test_split method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data set into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(numeric_features,target, test_size = 0.2, random_state = 10)\n",
    "\n",
    "#Creation of Train and validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=5)\n",
    "\n",
    "print (\"Train dataset: {0}{1}\".format(X_train.shape, y_train.shape))\n",
    "print (\"Validation dataset: {0}{1}\".format(X_val.shape, y_val.shape))\n",
    "print (\"Test dataset: {0}{1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's select few algorithm used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = ['LR','Random Forest','Neural Network','GaussianNB','DecisionTreeClassifier','SVM','KNN']\n",
    "\n",
    "models.append((LogisticRegression()))\n",
    "models.append((RandomForestClassifier(n_estimators=10)))\n",
    "models.append((MLPClassifier()))\n",
    "models.append((GaussianNB()))\n",
    "models.append((DecisionTreeClassifier()))\n",
    "models.append((SVC()))\n",
    "models.append((KNeighborsClassifier(n_neighbors=3)))\n",
    "\n",
    "print (models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run K-Cross Validation to Build the models and find the one with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10,random_state=7)\n",
    "\n",
    "for i in range(0,len(models)):    \n",
    "    cv_result = model_selection.cross_val_score(models[i],X_train,y_train,cv=kfold,scoring='accuracy')\n",
    "    print ('-'*40)\n",
    "    print ('{0}: {1}'.format(model_names[i],cv_result))\n",
    "    \n",
    "    trained_model=models[i].fit(X_train,y_train)\n",
    "    print ('-'*40)\n",
    "    print ('{0}: {1}'.format(model_names[i],trained_model))\n",
    "    \n",
    "    prediction = models[i].predict(X_val)\n",
    "    acc_score = accuracy_score(y_val,prediction)     \n",
    "    print ('-'*40)\n",
    "    print ('{0}: {1}'.format(model_names[i],acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's predict our test data and see prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = RandomForestClassifier(n_estimators=100)\n",
    "randomForestModel.fit(X_train,y_train)\n",
    "prediction = randomForestModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('-'*40)\n",
    "print ('Accuracy score:')\n",
    "print (accuracy_score(y_test,prediction))\n",
    "print ('-'*40)\n",
    "print ('Confusion Matrix:')\n",
    "print (confusion_matrix(y_test,prediction))\n",
    "print ('-'*40)\n",
    "print ('Classification Matrix:')\n",
    "print (classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualize model performance using a library called yellowbrick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#install yellowbrick\n",
    "!pip install yellowbrick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary module \n",
    "from yellowbrick.classifier import ClassificationReport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a classification report for the RandomForest algorithm\n",
    "\n",
    "I.\tInstantiate the Classification Report instance, passing in the RandomForest Object and the PREDICTION classes\n",
    "\n",
    "II.\tPass the training sets into the fit method \n",
    "\n",
    "III.\tPass the test sets into the score method\n",
    "\n",
    "IV.\tUsing the poof method to display the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(randomForestModel, classes=['<=50','>50'])\n",
    "\n",
    "#Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    " \n",
    "#Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test) \n",
    "\n",
    "# Draw/show/poof the data\n",
    "g = visualizer.poof() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pickle library\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_randomForestModel.mdl'\n",
    "pickle.dump(randomForestModel, open(filename, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'finalized_randomForestModel.mdl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)\n",
    "\n",
    "prediction = randomForestModel.predict(X_test)\n",
    "print ('-'*40)\n",
    "print ('Accuracy score:')\n",
    "print (accuracy_score(y_test,prediction))\n",
    "print ('-'*40)\n",
    "print ('Confusion Matrix:')\n",
    "print (confusion_matrix(y_test,prediction))\n",
    "print ('-'*40)\n",
    "print ('Classification Matrix:')\n",
    "print (classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "--\n",
    "What we have done in this notebook:\n",
    "\n",
    "* Find the API Docs for the running version of Pandas & scikit-learn\n",
    "* Run data exploration\n",
    "* Run data visualization\n",
    "* Run data preparation\n",
    "* Train models\n",
    "* Evaluate models\n",
    "* Save and load trained models\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
